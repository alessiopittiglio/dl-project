# Novel View Synthesis with NeRF

## Project Overview

The goal of this project is to develop a model for Novel View Synthesis. This involves generating new images of a 3D object from viewpoints that are not present in the initial set of images. The model is trained on RGB images and their corresponding camera poses.

## Dataset
The project uses the NeRF Synthetic Dataset provided by Cornell University, specifically the Lego scene.

Dataset structure:

```
nerf_synthetic/
└── SCENE_NAME/
    ├── train/
    ├── val/
    ├── test/
    ├── transforms_train.json
    ├── transforms_val.json
    └── transforms_test.json
```

## Technical Approach
The solution implements the NeRF paper from scratch. The model is Multi-Layer Perceptron (MLP). It takes the encoded position and viewing direction as input and predicts the RGB color and volume density ($\sigma$) at that point. The architecture is composed of 8 fully connected layers with a skip connection at the 4th layer.

## Training
A two-phase strategy was adopted: an initial phase of 50,000 iterations followed by a fine-tuning phase of 10,000 iterations. The model achieved a final PSNR of 30.3 on the test set.

## 360° View Synthesis

A 360° camera rotation generated by the trained NeRF model is shown below.

![NeRF Rotation](assets/rotation.gif)
